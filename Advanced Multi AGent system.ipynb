{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cbf6d3-f2f2-42bc-a32f-622286c7a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import langgraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "from IPython.display import Image, display\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "import operator\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.pool import StaticPool\n",
    "\n",
    "# Define model for Ollama\n",
    "local_llm = \"llama3.2:3b-instruct-fp16\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Define shared graph state\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    column_name_str: str\n",
    "    column_names: dict\n",
    "    chain: object\n",
    "    extracted_data: List[dict]\n",
    "    merged_data: pd.DataFrame\n",
    "    db_engine: object\n",
    "    max_retries: int\n",
    "    loop_step: Annotated[int, operator.add]\n",
    "    metadata: dict\n",
    "\n",
    "### Define Agents\n",
    "class ColumnNameAgent:\n",
    "    def run(self, state):\n",
    "        column_name_str = state[\"column_name_str\"]\n",
    "        column_names = {}\n",
    "        for name in column_name_str.split(\";\"):\n",
    "            col_name, desc = name.strip().split(':')\n",
    "            column_names[col_name] = f\"<{desc}>\"\n",
    "        state[\"column_names\"] = column_names\n",
    "        return state\n",
    "\n",
    "class ChainCreationAgent:\n",
    "    def run(self, state):\n",
    "        column_names = state[\"column_names\"]\n",
    "        template = \"\"\"You need to act as a Named Entity Recognizer.\n",
    "        Extract the following column names from the review text: {column_names}\n",
    "        \n",
    "        Review Text: \"{review}\"\n",
    "        \n",
    "        STRICTLY respond in JSON format like: {{\"column_1\": \"<value 1>\", ... }}\n",
    "        \"\"\"\n",
    "        output_parser = JsonOutputParser()\n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "        chain = prompt | llm | output_parser\n",
    "        state[\"chain\"] = chain\n",
    "        return state\n",
    "\n",
    "class EntityExtractionAgent:\n",
    "    def run(self, state):\n",
    "        chain = state[\"chain\"]\n",
    "        review_texts = state[\"df2\"][\"ReviewText\"].tolist()\n",
    "        column_names = state[\"column_names\"]\n",
    "        extracted_data = []\n",
    "        for review in review_texts:\n",
    "            response = chain.invoke({\"review\": review, \"column_names\": column_names})\n",
    "            extracted_data.append(response)\n",
    "        state[\"extracted_data\"] = extracted_data\n",
    "        return state\n",
    "\n",
    "class DataCombinationAgent:\n",
    "    def run(self, state):\n",
    "        df1 = state[\"df1\"]\n",
    "        extracted_data = state[\"extracted_data\"]\n",
    "        extracted_df = pd.DataFrame(extracted_data)\n",
    "        merged_data = pd.merge(df1, extracted_df, left_on=list(state[\"column_names\"].keys()), \n",
    "                               right_on=list(state[\"column_names\"].keys()), how='left')\n",
    "        merged_data.to_csv(\"merged_data.csv\", index=False)\n",
    "        state[\"merged_data\"] = merged_data\n",
    "        return state\n",
    "\n",
    "class DatabaseAgent:\n",
    "    def run(self, state):\n",
    "        merged_data = state[\"merged_data\"]\n",
    "        connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n",
    "        merged_data.to_sql('my_table', connection, index=False, if_exists='replace')\n",
    "        state[\"db_engine\"] = create_engine(\n",
    "            \"sqlite://\", creator=lambda: connection, poolclass=StaticPool,\n",
    "            connect_args={\"check_same_thread\": False}\n",
    "        )\n",
    "        return state\n",
    "\n",
    "class MetadataExtractionAgent:\n",
    "    def run(self, state):\n",
    "        df1 = state[\"df1\"]\n",
    "        df2 = state[\"df2\"]\n",
    "        metadata = {\n",
    "            \"df1\": {\n",
    "                \"columns\": df1.columns.tolist(),\n",
    "                \"dtypes\": df1.dtypes.apply(str).to_dict(),\n",
    "                \"num_rows\": len(df1),\n",
    "                \"missing_values\": df1.isnull().sum().to_dict()\n",
    "            },\n",
    "            \"df2\": {\n",
    "                \"columns\": df2.columns.tolist(),\n",
    "                \"dtypes\": df2.dtypes.apply(str).to_dict(),\n",
    "                \"num_rows\": len(df2),\n",
    "                \"missing_values\": df2.isnull().sum().to_dict()\n",
    "            }\n",
    "        }\n",
    "        state[\"metadata\"] = metadata\n",
    "        return state\n",
    "\n",
    "class GlobalAgent:\n",
    "    def __init__(self):\n",
    "        self.agents = {\n",
    "            \"ColumnNameAgent\": ColumnNameAgent(),\n",
    "            \"ChainCreationAgent\": ChainCreationAgent(),\n",
    "            \"EntityExtractionAgent\": EntityExtractionAgent(),\n",
    "            \"DataCombinationAgent\": DataCombinationAgent(),\n",
    "            \"DatabaseAgent\": DatabaseAgent(),\n",
    "            \"MetadataExtractionAgent\": MetadataExtractionAgent()\n",
    "        }\n",
    "\n",
    "    def run(self, agent_name, state):\n",
    "        if agent_name in self.agents:\n",
    "            return self.agents[agent_name].run(state)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown agent: {agent_name}\")\n",
    "\n",
    "    def monitor_agents(self):\n",
    "        # Placeholder for monitoring logic, e.g., logging, error handling, retries, etc.\n",
    "        print(\"Monitoring agents... All agents are operational.\")\n",
    "\n",
    "### Define Graph Nodes for Workflow Execution\n",
    "def execute_column_name_agent(state):\n",
    "    global_agent = GlobalAgent()\n",
    "    return global_agent.run(\"ColumnNameAgent\", state)\n",
    "\n",
    "def execute_chain_creation_agent(state):\n",
    "    global_agent = GlobalAgent()\n",
    "    return global_agent.run(\"ChainCreationAgent\", state)\n",
    "\n",
    "def execute_entity_extraction_agent(state):\n",
    "    global_agent = GlobalAgent()\n",
    "    return global_agent.run(\"EntityExtractionAgent\", state)\n",
    "\n",
    "def execute_data_combination_agent(state):\n",
    "    global_agent = GlobalAgent()\n",
    "    return global_agent.run(\"DataCombinationAgent\", state)\n",
    "\n",
    "def execute_database_agent(state):\n",
    "    global_agent = GlobalAgent()\n",
    "    return global_agent.run(\"DatabaseAgent\", state)\n",
    "\n",
    "def execute_metadata_extraction_agent(state):\n",
    "    global_agent = GlobalAgent()\n",
    "    return global_agent.run(\"MetadataExtractionAgent\", state)\n",
    "\n",
    "# Decision function to proceed based on step completion\n",
    "def decide_next_step(state):\n",
    "    if state[\"loop_step\"] < state[\"max_retries\"]:\n",
    "        return \"execute_database_agent\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "def decide_entry_point(state):\n",
    "    if \"column_name_str\" in state and state[\"column_name_str\"]:\n",
    "        return \"execute_chain_creation_agent\"\n",
    "    else:\n",
    "        return \"retry\"\n",
    "\n",
    "### Create Workflow Graph with Conditional Entry Points and Edges\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"execute_column_name_agent\", execute_column_name_agent)\n",
    "workflow.add_node(\"execute_chain_creation_agent\", execute_chain_creation_agent)\n",
    "workflow.add_node(\"execute_entity_extraction_agent\", execute_entity_extraction_agent)\n",
    "workflow.add_node(\"execute_data_combination_agent\", execute_data_combination_agent)\n",
    "workflow.add_node(\"execute_database_agent\", execute_database_agent)\n",
    "workflow.add_node(\"execute_metadata_extraction_agent\", execute_metadata_extraction_agent)\n",
    "\n",
    "# Adding GlobalAgent as a visual node for monitoring\n",
    "workflow.add_node(\"monitor_agents\", lambda state: GlobalAgent().monitor_agents())\n",
    "\n",
    "# Define the entry point and edges\n",
    "workflow.set_conditional_entry_point(\n",
    "    decide_entry_point,\n",
    "    {\n",
    "        \"execute_chain_creation_agent\": \"execute_chain_creation_agent\",\n",
    "        \"retry\": \"execute_column_name_agent\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"execute_column_name_agent\", \"monitor_agents\")\n",
    "workflow.add_edge(\"monitor_agents\", \"execute_chain_creation_agent\")\n",
    "workflow.add_edge(\"execute_chain_creation_agent\", \"execute_entity_extraction_agent\")\n",
    "workflow.add_edge(\"execute_entity_extraction_agent\", \"execute_data_combination_agent\")\n",
    "workflow.add_edge(\"execute_data_combination_agent\", \"execute_metadata_extraction_agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"execute_metadata_extraction_agent\",\n",
    "    decide_next_step,\n",
    "    {\n",
    "        \"execute_database_agent\": \"execute_database_agent\",\n",
    "        \"end_workflow\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile and display the graph\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Process function to execute workflow step by step\n",
    "def process_workflow(state):\n",
    "    global_agent = GlobalAgent()\n",
    "    global_agent.monitor_agents()  # Monitor agents before starting the workflow\n",
    "    next_node = \"execute_column_name_agent\"\n",
    "    while next_node != END:\n",
    "        if next_node == \"execute_column_name_agent\":\n",
    "            state = execute_column_name_agent(state)\n",
    "            next_node = \"monitor_agents\"\n",
    "        elif next_node == \"monitor_agents\":\n",
    "            global_agent.monitor_agents()\n",
    "            next_node = \"execute_chain_creation_agent\"\n",
    "        elif next_node == \"execute_chain_creation_agent\":\n",
    "            state = execute_chain_creation_agent(state)\n",
    "            next_node = \"execute_entity_extraction_agent\"\n",
    "        elif next_node == \"execute_entity_extraction_agent\":\n",
    "            state = execute_entity_extraction_agent(state)\n",
    "            next_node = \"execute_data_combination_agent\"\n",
    "        elif next_node == \"execute_data_combination_agent\":\n",
    "            state = execute_data_combination_agent(state)\n",
    "            next_node = \"execute_metadata_extraction_agent\"\n",
    "        elif next_node == \"execute_metadata_extraction_agent\":\n",
    "            state = execute_metadata_extraction_agent(state)\n",
    "            next_node = decide_next_step(state)\n",
    "        elif next_node == \"execute_database_agent\":\n",
    "            state = execute_database_agent(state)\n",
    "            next_node = END\n",
    "        else:\n",
    "            raise ValueError(\"Unknown next node: {}\".format(next_node))\n",
    "    return state\n",
    "\n",
    "# Initialize the state and run the workflow\n",
    "initial_state = GraphState(\n",
    "    question=\"What are the types of agent memory?\",\n",
    "    column_name_str=\"Name:Name of the customer;PurchaseDate:Date of purchase of product\",\n",
    "    column_names={},\n",
    "    chain=None,\n",
    "    extracted_data=[],\n",
    "    merged_data=pd.DataFrame(),\n",
    "    db_engine=None,\n",
    "    max_retries=3,\n",
    "    loop_step=0,\n",
    "    metadata={}\n",
    ")\n",
    "\n",
    "# Example data for df1 and df2 (replace these with actual data)\n",
    "initial_state[\"df1\"] = pd.DataFrame({\n",
    "    'Name': ['Alice', 'Bob'],\n",
    "    'PurchaseDate': ['2023-01-01', '2023-01-02']\n",
    "})\n",
    "\n",
    "initial_state[\"df2\"] = pd.DataFrame({\n",
    "    'ReviewText': ['Alice bought a product on 2023-01-01.', 'Bob bought a product on 2023-01-02.']\n",
    "})\n",
    "\n",
    "# Run the workflow step by step\n",
    "final_state = process_workflow(initial_state)\n",
    "\n",
    "# Output the result\n",
    "print(\"Final state:\", final_state)\n",
    "print(\"Database engine:\", final_state[\"db_engine\"])\n",
    "print(\"Metadata:\", final_state[\"metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb860f6-e0d1-4e4d-b626-e4b63bb36134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import operator\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_ollama import ChatOllama  # Import Ollama LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain  # Import LLMChain\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_core.memory import BaseMemory\n",
    "\n",
    "# Define model using Ollama\n",
    "local_llm = \"llama3.2:3b-instruct-fp16\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Define shared graph state\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    column_name_str: str\n",
    "    column_names: dict\n",
    "    chain: object\n",
    "    extracted_data: List[dict]\n",
    "    merged_data: pd.DataFrame\n",
    "    metadata: dict\n",
    "    max_retries: int\n",
    "    loop_step: Annotated[int, operator.add]\n",
    "    db_engine: sqlite3.Connection\n",
    "    df1: Dict\n",
    "    df2: Dict\n",
    "\n",
    "# Stateful Memory Class\n",
    "class StatefulMemory(BaseMemory):\n",
    "    state: Dict = {}\n",
    "\n",
    "    @property\n",
    "    def memory_variables(self):\n",
    "        return list(self.state.keys())\n",
    "\n",
    "    def load_memory_variables(self, inputs):\n",
    "        return self.state\n",
    "\n",
    "    def save_context(self, inputs, outputs):\n",
    "        pass  # We handle state updates manually\n",
    "\n",
    "    def clear(self):\n",
    "        self.state = {}\n",
    "\n",
    "# Initialize memory\n",
    "memory = StatefulMemory()\n",
    "\n",
    "# SQLite database setup\n",
    "connection = sqlite3.connect(\":memory:\")  # Create an in-memory SQLite database\n",
    "memory.state[\"db_engine\"] = connection  # Save the database engine in state\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Create tables for storing results\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS merged_data (\n",
    "    CustomerName TEXT,\n",
    "    PurchaseDate TEXT\n",
    ")\n",
    "\"\"\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS metadata (\n",
    "    table_name TEXT,\n",
    "    key TEXT,\n",
    "    value TEXT\n",
    ")\n",
    "\"\"\")\n",
    "connection.commit()\n",
    "\n",
    "# Function to store data in SQLite\n",
    "def store_in_sqlite(table_name: str, data: pd.DataFrame):\n",
    "    data.to_sql(table_name, connection, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Tools Definition\n",
    "def column_name_extraction(input_text: str) -> str:\n",
    "    state = memory.state\n",
    "    input_text = input_text.strip('\"').strip()\n",
    "    column_names = {}\n",
    "    for name in input_text.split(\";\"):\n",
    "        if ':' in name:\n",
    "            col_name, desc = name.strip().split(':', 1)\n",
    "            column_names[col_name.strip()] = f\"<{desc.strip()}>\"\n",
    "        else:\n",
    "            return \"Invalid format in column_name_str.\"\n",
    "    state[\"column_names\"] = column_names\n",
    "    return f\"Column names extracted: {list(column_names.keys())}\"\n",
    "\n",
    "column_name_tool = Tool(\n",
    "    name=\"ColumnNameExtraction\",\n",
    "    func=column_name_extraction,\n",
    "    description=\"Extracts column names from a string formatted as 'ColumnName:Description;ColumnName:Description'.\"\n",
    ")\n",
    "\n",
    "def chain_creation(input_text: str) -> str:\n",
    "    state = memory.state\n",
    "    column_names = state.get(\"column_names\", {})\n",
    "    if not column_names:\n",
    "        return \"Error: 'column_names' not found in state.\"\n",
    "    template = f\"\"\"You are a Named Entity Recognizer.\n",
    "Given the following review text, extract the entities corresponding to the specified column names.\n",
    "\n",
    "Column Names and Descriptions: {column_names}\n",
    "\n",
    "Review Text: \"{{{{review}}}}\"\n",
    "\n",
    "Respond in JSON format with the extracted entities, e.g., {{\"column_1\": \"value1\", \"column_2\": \"value2\"}}\n",
    "\"\"\"\n",
    "    try:\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"review\"])\n",
    "        chain = LLMChain(prompt=prompt, llm=llm)\n",
    "        state[\"chain\"] = chain\n",
    "        return \"Chain created for entity extraction\"\n",
    "    except Exception as e:\n",
    "        return f\"Error in chain creation: {str(e)}\"\n",
    "\n",
    "chain_creation_tool = Tool(\n",
    "    name=\"ChainCreation\",\n",
    "    func=chain_creation,\n",
    "    description=\"Creates a chain for entity extraction based on the column names in the state.\"\n",
    ")\n",
    "\n",
    "def entity_extraction(input_text: str) -> str:\n",
    "    state = memory.state\n",
    "    if \"df2\" not in state or not state[\"df2\"]:\n",
    "        return \"Error: 'df2' not found in state.\"\n",
    "\n",
    "    chain = state.get(\"chain\")\n",
    "    if not chain:\n",
    "        return \"Error: 'chain' not found in state.\"\n",
    "    \n",
    "    df2 = pd.DataFrame(state[\"df2\"])\n",
    "    if \"ReviewText\" not in df2.columns:\n",
    "        return \"Error: 'ReviewText' column not found in df2.\"\n",
    "\n",
    "    review_texts = df2[\"ReviewText\"].tolist()\n",
    "    extracted_data = []\n",
    "    for review in review_texts:\n",
    "        try:\n",
    "            response = chain.run({\"review\": review})\n",
    "            extracted_entities = json.loads(response)\n",
    "            extracted_data.append(extracted_entities)\n",
    "        except Exception as e:\n",
    "            extracted_data.append({})\n",
    "    state[\"extracted_data\"] = extracted_data\n",
    "    return \"Entities extracted from review texts\"\n",
    "\n",
    "entity_extraction_tool = Tool(\n",
    "    name=\"EntityExtraction\",\n",
    "    func=entity_extraction,\n",
    "    description=\"Extracts entities from review texts using the chain created.\"\n",
    ")\n",
    "\n",
    "def data_combination(input_text: str) -> str:\n",
    "    state = memory.state\n",
    "    if \"df1\" not in state or not state[\"df1\"]:\n",
    "        return \"Error: 'df1' not found in state.\"\n",
    "\n",
    "    df1 = pd.DataFrame(state[\"df1\"])\n",
    "    extracted_data = state.get(\"extracted_data\", [])\n",
    "    if not extracted_data:\n",
    "        return \"Error: 'extracted_data' not found in state.\"\n",
    "    \n",
    "    extracted_df = pd.DataFrame(extracted_data)\n",
    "    column_keys = list(df1.columns)\n",
    "    if not all(key in extracted_df.columns for key in column_keys):\n",
    "        for key in column_keys:\n",
    "            if key not in extracted_df.columns:\n",
    "                extracted_df[key] = None\n",
    "\n",
    "    merged_data = pd.merge(df1, extracted_df, how='left')\n",
    "    state[\"merged_data\"] = merged_data.to_dict(orient='records')  # Use 'records' for the desired output\n",
    "    # Store merged data in SQLite\n",
    "    store_in_sqlite(\"merged_data\", pd.DataFrame(state[\"merged_data\"]))\n",
    "    return \"Data combined and merged data updated in state.\"\n",
    "\n",
    "data_combination_tool = Tool(\n",
    "    name=\"DataCombination\",\n",
    "    func=data_combination,\n",
    "    description=\"Combines the extracted data with the original dataframe.\"\n",
    ")\n",
    "\n",
    "def metadata_extraction(input_text: str) -> str:\n",
    "    state = memory.state\n",
    "    df1 = pd.DataFrame(state[\"df1\"])\n",
    "    df2 = pd.DataFrame(state[\"df2\"])\n",
    "    metadata = {\n",
    "        \"df1\": {\n",
    "            \"columns\": df1.columns.tolist(),\n",
    "            \"dtypes\": df1.dtypes.apply(str).to_dict(),\n",
    "            \"num_rows\": len(df1),\n",
    "            \"missing_values\": df1.isnull().sum().to_dict(),\n",
    "            \"statistical_summary\": df1.describe(include=\"all\").to_dict()\n",
    "        },\n",
    "        \"df2\": {\n",
    "            \"columns\": df2.columns.tolist(),\n",
    "            \"dtypes\": df2.dtypes.apply(str).to_dict(),\n",
    "            \"num_rows\": len(df2),\n",
    "            \"missing_values\": df2.isnull().sum().to_dict(),\n",
    "            \"statistical_summary\": df2.describe(include=\"all\").to_dict()\n",
    "        }\n",
    "    }\n",
    "    state[\"metadata\"] = metadata\n",
    "\n",
    "    # Serialize metadata and store in SQLite\n",
    "    def serialize_metadata(metadata: dict) -> pd.DataFrame:\n",
    "        flat_metadata = []\n",
    "        for table, details in metadata.items():\n",
    "            for key, value in details.items():\n",
    "                if isinstance(value, (list, dict)):\n",
    "                    value = json.dumps(value)  # Serialize lists and dicts to JSON strings\n",
    "                flat_metadata.append({\"table_name\": table, \"key\": key, \"value\": value})\n",
    "        return pd.DataFrame(flat_metadata)\n",
    "\n",
    "    metadata_df = serialize_metadata(metadata)\n",
    "    store_in_sqlite(\"metadata\", metadata_df)\n",
    "    return \"Metadata extracted and added to state.\"\n",
    "\n",
    "metadata_extraction_tool = Tool(\n",
    "    name=\"MetadataExtraction\",\n",
    "    func=metadata_extraction,\n",
    "    description=\"Extracts metadata from df1 and df2.\"\n",
    ")\n",
    "\n",
    "# Define the custom prompt\n",
    "custom_prompt = \"\"\"\n",
    "You are a helpful assistant that uses tools to answer questions.\n",
    "\n",
    "When responding, always use the following format:\n",
    "Thought: [Your thought process]\n",
    "Action: [The tool name]\n",
    "Action Input: [The input to the tool. Do not leave this empty!]\n",
    "\n",
    "If you have enough information to provide a final answer, use this format:\n",
    "Thought: [Your thought process]\n",
    "Final Answer: [Your answer]\n",
    "\n",
    "Ensure every Action includes a valid and properly formatted Action Input.\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[\n",
    "        column_name_tool,\n",
    "        chain_creation_tool,\n",
    "        entity_extraction_tool,\n",
    "        data_combination_tool,\n",
    "        metadata_extraction_tool\n",
    "    ],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    max_iterations=5,\n",
    "    agent_kwargs={\"agent_prompt\": custom_prompt},\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Global Agent\n",
    "def global_agent(state):\n",
    "    max_retries = 3  # Set the maximum number of retries for failed steps\n",
    "    try:\n",
    "        print(\"> Starting global agent...\")\n",
    "        next_agent = \"execute_column_name_agent\"\n",
    "\n",
    "        while next_agent != END:\n",
    "            if next_agent == \"execute_column_name_agent\":\n",
    "                state = execute_column_name_agent(state)\n",
    "                next_agent = \"execute_chain_creation_agent\"\n",
    "            elif next_agent == \"execute_chain_creation_agent\":\n",
    "                state = execute_chain_creation_agent(state)\n",
    "                next_agent = \"execute_entity_extraction_agent\"\n",
    "            elif next_agent == \"execute_entity_extraction_agent\":\n",
    "                state = execute_entity_extraction_agent(state)\n",
    "                next_agent = \"execute_data_combination_agent\"\n",
    "            elif next_agent == \"execute_data_combination_agent\":\n",
    "                state = execute_data_combination_agent(state)\n",
    "                next_agent = \"execute_metadata_extraction_agent\"\n",
    "            elif next_agent == \"execute_metadata_extraction_agent\":\n",
    "                state = execute_metadata_extraction_agent(state)\n",
    "                next_agent = END\n",
    "\n",
    "        print(\"> Finished chain.\")\n",
    "        print(\"Global agent completed successfully.\")\n",
    "        final_answer = state.get(\"merged_data\", {})\n",
    "        print(f\"Final Answer: {final_answer}\")\n",
    "        return final_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in global agent: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example Data\n",
    "initial_state = GraphState(\n",
    "    question=\"Which customers purchased products\",\n",
    "    column_name_str=\"CustomerName:Name of the customer;PurchaseDate:Date of purchase of product\",\n",
    "    column_names={},  # Will be populated correctly by the workflow\n",
    "    chain=None,\n",
    "    extracted_data=[],\n",
    "    merged_data=pd.DataFrame(),\n",
    "    metadata={},\n",
    "    max_retries=3,\n",
    "    loop_step=0,\n",
    "    df1={\n",
    "        'CustomerName': ['Alice', 'Bob'],\n",
    "        'PurchaseDate': ['2023-01-01', '2023-01-02']\n",
    "    },\n",
    "    df2={\n",
    "        'ReviewText': [\n",
    "            \"Alice purchased a laptop on 2023-01-01.\",\n",
    "            \"Bob purchased a smartphone on 2023-01-02.\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "memory.state = initial_state  # Set the initial state in memory\n",
    "\n",
    "# Run the Global Agent\n",
    "final_result = global_agent(initial_state)\n",
    "\n",
    "# Verify SQLite content\n",
    "print(\"\\nData from SQLite - Merged Data:\")\n",
    "print(pd.read_sql_query(\"SELECT * FROM merged_data\", connection))\n",
    "\n",
    "print(\"\\nData from SQLite - Metadata:\")\n",
    "print(pd.read_sql_query(\"SELECT * FROM metadata\", connection))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528c8725-9e8e-43ee-89db-461140ed052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import operator\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_ollama import ChatOllama  # Import Ollama LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain  # Import LLMChain\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain_core.memory import BaseMemory\n",
    "\n",
    "# Define model using Ollama\n",
    "local_llm = \"llama3.2:3b-instruct-fp16\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Define shared graph state\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    column_name_str: str\n",
    "    column_names: dict\n",
    "    chain: object\n",
    "    extracted_data: List[dict]\n",
    "    merged_data: pd.DataFrame\n",
    "    metadata: dict\n",
    "    max_retries: int\n",
    "    loop_step: Annotated[int, operator.add]\n",
    "    db_engine: sqlite3.Connection\n",
    "    df1: Dict\n",
    "    df2: Dict\n",
    "\n",
    "# Stateful Memory Class\n",
    "class StatefulMemory(BaseMemory):\n",
    "    state: Dict = {}\n",
    "\n",
    "    @property\n",
    "    def memory_variables(self):\n",
    "        return list(self.state.keys())\n",
    "\n",
    "    def load_memory_variables(self, inputs):\n",
    "        return self.state\n",
    "\n",
    "    def save_context(self, inputs, outputs):\n",
    "        pass  # We handle state updates manually\n",
    "\n",
    "    def clear(self):\n",
    "        self.state = {}\n",
    "\n",
    "# Initialize memory\n",
    "memory = StatefulMemory()\n",
    "\n",
    "# SQLite database setup\n",
    "connection = sqlite3.connect(\":memory:\")  # Create an in-memory SQLite database\n",
    "memory.state[\"db_engine\"] = connection  # Save the database engine in state\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Create tables for storing results\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS merged_data (\n",
    "    CustomerName TEXT,\n",
    "    PurchaseDate TEXT\n",
    ")\n",
    "\"\"\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS metadata (\n",
    "    key TEXT,\n",
    "    value TEXT\n",
    ")\n",
    "\"\"\")\n",
    "connection.commit()\n",
    "\n",
    "# Function to store data in SQLite\n",
    "def store_in_sqlite(table_name: str, data: pd.DataFrame):\n",
    "    data.to_sql(table_name, connection, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Tools Definition\n",
    "def column_name_extraction(input_text: str) -> str:\n",
    "    state = memory.state\n",
    "    input_text = input_text.strip('\"').strip()\n",
    "    column_names = {}\n",
    "    for name in input_text.split(\";\"):\n",
    "        if ':' in name:\n",
    "            col_name, desc = name.strip().split(':', 1)\n",
    "            column_names[col_name.strip()] = f\"<{desc.strip()}>\"\n",
    "        else:\n",
    "            return \"Invalid format in column_name_str.\"\n",
    "    state[\"column_names\"] = column_names\n",
    "    return f\"Column names extracted: {list(column_names.keys())}\"\n",
    "\n",
    "column_name_tool = Tool(\n",
    "    name=\"ColumnNameExtraction\",\n",
    "    func=column_name_extraction,\n",
    "    description=\"Extracts column names from a string formatted as 'ColumnName:Description;ColumnName:Description'.\"\n",
    ")\n",
    "\n",
    "def chain_creation(input_text: str) -> str:\n",
    "    state = memory.state\n",
    "    column_names = state.get(\"column_names\", {})\n",
    "    if not column_names:\n",
    "        return \"Error: 'column_names' not found in state.\"\n",
    "    template = f\"\"\"You are a Named Entity Recognizer.\n",
    "Given the following review text, extract the entities corresponding to the specified column names.\n",
    "\n",
    "Column Names and Descriptions: {column_names}\n",
    "\n",
    "Review Text: \"{{{{review}}}}\"\n",
    "\n",
    "Respond in JSON format with the extracted entities, e.g., {{\"column_1\": \"value1\", \"column_2\": \"value2\"}}\n",
    "\"\"\"\n",
    "    try:\n",
    "        prompt = PromptTemplate(template=template, input_variables=[\"review\"])\n",
    "        chain = LLMChain(prompt=prompt, llm=llm)\n",
    "        state[\"chain\"] = chain\n",
    "        return \"Chain created for entity extraction\"\n",
    "    except Exception as e:\n",
    "        return f\"Error in chain creation: {str(e)}\"\n",
    "\n",
    "chain_creation_tool = Tool(\n",
    "    name=\"ChainCreation\",\n",
    "    func=chain_creation,\n",
    "    description=\"Creates a chain for entity extraction based on the column names in the state.\"\n",
    ")\n",
    "\n",
    "def entity_extraction(input_text: str) -> str:\n",
    "    state = memory.state\n",
    "    if \"df2\" not in state or not state[\"df2\"]:\n",
    "        return \"Error: 'df2' not found in state.\"\n",
    "\n",
    "    chain = state.get(\"chain\")\n",
    "    if not chain:\n",
    "        return \"Error: 'chain' not found in state.\"\n",
    "    \n",
    "    df2 = pd.DataFrame(state[\"df2\"])\n",
    "    if \"ReviewText\" not in df2.columns:\n",
    "        return \"Error: 'ReviewText' column not found in df2.\"\n",
    "\n",
    "    review_texts = df2[\"ReviewText\"].tolist()\n",
    "    extracted_data = []\n",
    "    for review in review_texts:\n",
    "        try:\n",
    "            response = chain.run({\"review\": review})\n",
    "            extracted_entities = json.loads(response)\n",
    "            extracted_data.append(extracted_entities)\n",
    "        except Exception as e:\n",
    "            extracted_data.append({})\n",
    "    state[\"extracted_data\"] = extracted_data\n",
    "    return \"Entities extracted from review texts\"\n",
    "\n",
    "entity_extraction_tool = Tool(\n",
    "    name=\"EntityExtraction\",\n",
    "    func=entity_extraction,\n",
    "    description=\"Extracts entities from review texts using the chain created.\"\n",
    ")\n",
    "\n",
    "def data_combination(input_text: str) -> str:\n",
    "    state = memory.state\n",
    "    if \"df1\" not in state or not state[\"df1\"]:\n",
    "        return \"Error: 'df1' not found in state.\"\n",
    "\n",
    "    df1 = pd.DataFrame(state[\"df1\"])\n",
    "    extracted_data = state.get(\"extracted_data\", [])\n",
    "    if not extracted_data:\n",
    "        return \"Error: 'extracted_data' not found in state.\"\n",
    "    \n",
    "    extracted_df = pd.DataFrame(extracted_data)\n",
    "    column_keys = list(df1.columns)\n",
    "    if not all(key in extracted_df.columns for key in column_keys):\n",
    "        for key in column_keys:\n",
    "            if key not in extracted_df.columns:\n",
    "                extracted_df[key] = None\n",
    "\n",
    "    merged_data = pd.merge(df1, extracted_df, how='left')\n",
    "    state[\"merged_data\"] = merged_data.to_dict(orient='records')  # Use 'records' for the desired output\n",
    "    # Store merged data in SQLite\n",
    "    store_in_sqlite(\"merged_data\", pd.DataFrame(state[\"merged_data\"]))\n",
    "    return \"Data combined and merged data updated in state.\"\n",
    "\n",
    "data_combination_tool = Tool(\n",
    "    name=\"DataCombination\",\n",
    "    func=data_combination,\n",
    "    description=\"Combines the extracted data with the original dataframe.\"\n",
    ")\n",
    "\n",
    "def metadata_extraction(input_text: str) -> str:\n",
    "    state = memory.state\n",
    "    df1 = pd.DataFrame(state[\"df1\"])\n",
    "    df2 = pd.DataFrame(state[\"df2\"])\n",
    "    metadata = {\n",
    "        \"df1\": {\n",
    "            \"columns\": df1.columns.tolist(),\n",
    "            \"dtypes\": df1.dtypes.apply(str).to_dict(),\n",
    "            \"num_rows\": len(df1),\n",
    "            \"missing_values\": df1.isnull().sum().to_dict(),\n",
    "            \"statistical_summary\": df1.describe(include=\"all\").to_dict()\n",
    "        },\n",
    "        \"df2\": {\n",
    "            \"columns\": df2.columns.tolist(),\n",
    "            \"dtypes\": df2.dtypes.apply(str).to_dict(),\n",
    "            \"num_rows\": len(df2),\n",
    "            \"missing_values\": df2.isnull().sum().to_dict(),\n",
    "            \"statistical_summary\": df2.describe(include=\"all\").to_dict()\n",
    "        }\n",
    "    }\n",
    "    state[\"metadata\"] = metadata\n",
    "    # Store metadata in SQLite\n",
    "    metadata_df = pd.DataFrame.from_dict(metadata, orient=\"index\").reset_index()\n",
    "    store_in_sqlite(\"metadata\", metadata_df)\n",
    "    return \"Metadata extracted and added to state.\"\n",
    "\n",
    "metadata_extraction_tool = Tool(\n",
    "    name=\"MetadataExtraction\",\n",
    "    func=metadata_extraction,\n",
    "    description=\"Extracts metadata from df1 and df2.\"\n",
    ")\n",
    "\n",
    "# Define the custom prompt\n",
    "custom_prompt = \"\"\"\n",
    "You are a helpful assistant that uses tools to answer questions.\n",
    "\n",
    "When responding, always use the following format:\n",
    "Thought: [Your thought process]\n",
    "Action: [The tool name]\n",
    "Action Input: [The input to the tool. Do not leave this empty!]\n",
    "\n",
    "If you have enough information to provide a final answer, use this format:\n",
    "Thought: [Your thought process]\n",
    "Final Answer: [Your answer]\n",
    "\n",
    "Ensure every Action includes a valid and properly formatted Action Input.\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[\n",
    "        column_name_tool,\n",
    "        chain_creation_tool,\n",
    "        entity_extraction_tool,\n",
    "        data_combination_tool,\n",
    "        metadata_extraction_tool\n",
    "    ],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    max_iterations=5,\n",
    "    agent_kwargs={\"agent_prompt\": custom_prompt},\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Define individual agents\n",
    "def execute_column_name_agent(state):\n",
    "    input_text = state[\"column_name_str\"]\n",
    "    agent.run(input_text)\n",
    "    return memory.state\n",
    "\n",
    "def execute_chain_creation_agent(state):\n",
    "    if \"column_names\" not in state or not state[\"column_names\"]:\n",
    "        raise ValueError(\"Error: 'column_names' is missing or empty in the state.\")\n",
    "    \n",
    "    input_text = f\"Create a chain for entity extraction using these column names: {state['column_names']}\"\n",
    "    agent.run(input_text)\n",
    "    return memory.state\n",
    "\n",
    "def execute_entity_extraction_agent(state):\n",
    "    if \"df2\" not in state or not state[\"df2\"]:\n",
    "        raise ValueError(\"Error: 'df2' is missing or empty in the state.\")\n",
    "    \n",
    "    review_texts = pd.DataFrame(state[\"df2\"]).get(\"ReviewText\", []).tolist()\n",
    "    if not review_texts:\n",
    "        raise ValueError(\"Error: 'ReviewText' column is missing or empty in df2.\")\n",
    "    \n",
    "    input_text = f\"Extract entities from the following review texts: {review_texts}\"\n",
    "    agent.run(input_text)\n",
    "    return memory.state\n",
    "\n",
    "def execute_data_combination_agent(state):\n",
    "    input_text = \"Combine extracted data with the original dataframe.\"\n",
    "    agent.run(input_text)\n",
    "    return memory.state\n",
    "\n",
    "def execute_metadata_extraction_agent(state):\n",
    "    input_text = \"Extract metadata from the dataframes.\"\n",
    "    agent.run(input_text)\n",
    "    return memory.state\n",
    "\n",
    "# Global Agent\n",
    "def global_agent(state):\n",
    "    max_retries = 3  # Set the maximum number of retries for failed steps\n",
    "    try:\n",
    "        print(\"> Starting global agent...\")\n",
    "        next_agent = \"execute_column_name_agent\"\n",
    "\n",
    "        while next_agent != END:\n",
    "            if next_agent == \"execute_column_name_agent\":\n",
    "                if \"column_name_str\" not in state or not state[\"column_name_str\"]:\n",
    "                    raise ValueError(\"State validation failed: 'column_name_str' is missing.\")\n",
    "                for _ in range(max_retries):  # Retry logic\n",
    "                    try:\n",
    "                        state = execute_column_name_agent(state)\n",
    "                        next_agent = \"execute_chain_creation_agent\"\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Retry failed: {e}\")\n",
    "                        continue\n",
    "\n",
    "            elif next_agent == \"execute_chain_creation_agent\":\n",
    "                if \"column_names\" not in state or not state[\"column_names\"]:\n",
    "                    raise ValueError(\"State validation failed: 'column_names' are missing.\")\n",
    "                for _ in range(max_retries):\n",
    "                    try:\n",
    "                        state = execute_chain_creation_agent(state)\n",
    "                        next_agent = \"execute_entity_extraction_agent\"\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Retry failed: {e}\")\n",
    "                        continue\n",
    "\n",
    "            elif next_agent == \"execute_entity_extraction_agent\":\n",
    "                if \"df2\" not in state or not state[\"df2\"]:\n",
    "                    raise ValueError(\"State validation failed: 'df2' is missing.\")\n",
    "                for _ in range(max_retries):\n",
    "                    try:\n",
    "                        state = execute_entity_extraction_agent(state)\n",
    "                        next_agent = \"execute_data_combination_agent\"\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Retry failed: {e}\")\n",
    "                        continue\n",
    "\n",
    "            elif next_agent == \"execute_data_combination_agent\":\n",
    "                if \"df1\" not in state or not state[\"df1\"]:\n",
    "                    raise ValueError(\"State validation failed: 'df1' is missing.\")\n",
    "                for _ in range(max_retries):\n",
    "                    try:\n",
    "                        state = execute_data_combination_agent(state)\n",
    "                        next_agent = \"execute_metadata_extraction_agent\"\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Retry failed: {e}\")\n",
    "                        continue\n",
    "\n",
    "            elif next_agent == \"execute_metadata_extraction_agent\":\n",
    "                for _ in range(max_retries):\n",
    "                    try:\n",
    "                        state = execute_metadata_extraction_agent(state)\n",
    "                        next_agent = END\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"Retry failed: {e}\")\n",
    "                        continue\n",
    "\n",
    "        print(\"> Finished chain.\")\n",
    "        print(\"Global agent completed successfully.\")\n",
    "        final_answer = state.get(\"merged_data\", {})\n",
    "        print(f\"Final Answer: {final_answer}\")\n",
    "        return final_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in global agent: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example Data\n",
    "initial_state = GraphState(\n",
    "    question=\"Which customers purchased products\",\n",
    "    column_name_str=\"CustomerName:Name of the customer;PurchaseDate:Date of purchase of product\",\n",
    "    column_names={},  # Will be populated correctly by the workflow\n",
    "    chain=None,\n",
    "    extracted_data=[],\n",
    "    merged_data=pd.DataFrame(),\n",
    "    metadata={},\n",
    "    max_retries=3,\n",
    "    loop_step=0,\n",
    "    df1={\n",
    "        'CustomerName': ['Alice', 'Bob'],\n",
    "        'PurchaseDate': ['2023-01-01', '2023-01-02']\n",
    "    },\n",
    "    df2={\n",
    "        'ReviewText': [\n",
    "            \"Alice purchased a laptop on 2023-01-01.\",\n",
    "            \"Bob purchased a smartphone on 2023-01-02.\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "memory.state = initial_state  # Set the initial state in memory\n",
    "\n",
    "# Run the Global Agent\n",
    "final_result = global_agent(initial_state)\n",
    "\n",
    "# Verify SQLite content\n",
    "print(\"\\nData from SQLite - Merged Data:\")\n",
    "print(pd.read_sql_query(\"SELECT * FROM merged_data\", connection))\n",
    "\n",
    "print(\"\\nData from SQLite - Metadata:\")\n",
    "print(pd.read_sql_query(\"SELECT * FROM metadata\", connection))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
